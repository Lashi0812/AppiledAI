{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q1 Can you explain what gradient descent is and how it works?\n",
    "1. Gradient descent is an optimization algorithm used to find the minimum of a function. It works by starting at a random point on the function and iteratively moving in the direction of steepest descent, or the direction that reduces the value of the function the most. At each step, the algorithm updates its current position by taking a step in the direction of the negative gradient of the function at the current position. This process is repeated until the algorithm reaches a local or global minimum of the function.\n",
    "2. The gradient of a function is a vector that points in the direction of the steepest increase in the value of the function. Therefore, the negative gradient points in the direction of the steepest decrease in the value of the function. The size of the step taken in this direction is determined by the learning rate, which is a hyperparameter of the algorithm. By taking small steps in the direction of the negative gradient, the gradient descent algorithm is able to gradually reduce the value of the function and find its minimum.\n",
    "3. The learning rate is a hyperparameter of the algorithm that determines the size of the steps taken in the direction of the negative gradient. A large learning rate may cause the algorithm to overshoot the minimum, while a small learning rate may slow down the convergence of the algorithm.\n",
    "4. The convergence of gradient descent depends on the shape of the cost function, which may have local minima or saddle points. A local minimum is a point where the value of the function is lower than the values of the function in the surrounding points, while a saddle point is a point where the function has different slopes in different directions.\n",
    "5. Stochastic gradient descent is a variation of gradient descent that uses a random sample of the training data, rather than the entire dataset, to compute the gradient at each step. This can make the algorithm faster and more efficient, but also more noisy and less stable.\n",
    "6. Gradient descent is a widely used optimization algorithm in the field of machine learning, where it is used to train various types of models, such as linear regression, logistic regression, or neural networks. It is also used in other fields, such as optimization of control systems or structural engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x = -2.000\n",
      "Minimum cost = 1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# define the cost function\n",
    "def cost_function(x):\n",
    "    return x ** 2 + 4 * x + 5\n",
    "\n",
    "\n",
    "# compute the gradient of the cost function\n",
    "def gradient(x):\n",
    "    return 2 * x + 4\n",
    "\n",
    "\n",
    "# set the initial position and the learning rate\n",
    "x = 0\n",
    "learning_rate = 0.1\n",
    "\n",
    "# repeat the gradient descent update until convergence\n",
    "while True:\n",
    "    # compute the gradient and the cost at the current position\n",
    "    grad = gradient(x)\n",
    "    cost = cost_function(x)\n",
    "\n",
    "    # check for convergence\n",
    "    if np.abs(grad) < 1e-3:\n",
    "        break\n",
    "\n",
    "    # update the position using the gradient and the learning rate\n",
    "    x -= learning_rate * grad\n",
    "\n",
    "# print the final position and the minimum cost\n",
    "print(f\"Minimum found at x = {x:.3f}\")\n",
    "print(f\"Minimum cost = {cost_function(x):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q2 How does the learning rate hyperparameter affect the performance of gradient descent?\n",
    "\n",
    "1. The learning rate is a hyperparameter of the gradient descent algorithm that determines the size of the steps taken in the direction of the negative gradient. A large learning rate may cause the algorithm to overshoot the minimum and diverge, while a small learning rate may slow down the convergence of the algorithm. Therefore, the learning rate should be set carefully to ensure the convergence of the algorithm.\n",
    "2. If the learning rate is too large, the algorithm may oscillate around the minimum or even diverge, as it will take large steps that may overshoot the minimum. This can make the algorithm unstable and prevent it from converging. On the other hand, if the learning rate is too small, the algorithm may converge slowly, as it will take small steps that may not reduce the value of the function quickly enough. This can make the algorithm inefficient and require a large number of iterations to find the minimum.\n",
    "3. To avoid these problems, it is recommended to use an adaptive learning rate that decreases over time. This can help the algorithm to take larger steps at the beginning, when it is far from the minimum, and smaller steps as it approaches the minimum, ensuring convergence and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 the cost is 5 and gradient is 4 \n",
      "iteration 1 the cost is 5.0 and gradient is -4.0 \n",
      "iteration 2 the cost is 5.0 and gradient is 4.0 \n",
      "iteration 3 the cost is 5.0 and gradient is -4.0 \n",
      "iteration 4 the cost is 5.0 and gradient is 4.0 \n",
      "iteration 5 the cost is 5.0 and gradient is -4.0 \n",
      "iteration 6 the cost is 5.0 and gradient is 4.0 \n",
      "iteration 7 the cost is 5.0 and gradient is -4.0 \n",
      "iteration 8 the cost is 5.0 and gradient is 4.0 \n",
      "iteration 9 the cost is 5.0 and gradient is -4.0 \n",
      "iteration 10 the cost is 5.0 and gradient is 4.0 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# define the cost function\n",
    "def cost_function(x):\n",
    "    return x ** 2 + 4 * x + 5\n",
    "\n",
    "\n",
    "# compute the gradient of the cost function\n",
    "def gradient(x):\n",
    "    return 2 * x + 4\n",
    "\n",
    "\n",
    "# set the initial position and the learning rate\n",
    "x = i = 0\n",
    "learning_rate = 1.0\n",
    "max_iter = 10\n",
    "\n",
    "# create a list to store the cost at each iteration\n",
    "costs = []\n",
    "\n",
    "# repeat the gradient descent update until convergence\n",
    "while True:\n",
    "    # compute the gradient and the cost at the current position\n",
    "    grad = gradient(x)\n",
    "    cost = cost_function(x)\n",
    "\n",
    "    # store the cost in the list\n",
    "    costs.append(cost)\n",
    "\n",
    "    # check for convergence\n",
    "    if np.abs(grad) < 1e-3 or i > max_iter:\n",
    "        break\n",
    "    print(f\"iteration {i} the cost is {cost} and gradient is {grad} \")\n",
    "    # update the position using the gradient and the learning rate\n",
    "    x -= learning_rate * grad\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the cost decreases quickly at the beginning, but then starts to oscillate around the minimum, indicating that the learning rate is too large and the algorithm is unstable. This can prevent the algorithm from converging to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 the cost is 5.0000 and gradient is 4.0000 \n",
      "iteration 1 the cost is 4.8416 and gradient is -3.9200 \n",
      "iteration 2 the cost is 4.5419 and gradient is 3.7640 \n",
      "iteration 3 the cost is 4.1336 and gradient is -3.5404 \n",
      "iteration 4 the cost is 3.6592 and gradient is 3.2614 \n",
      "iteration 5 the cost is 3.1634 and gradient is -2.9417 \n",
      "iteration 6 the cost is 2.6866 and gradient is 2.5974 \n",
      "iteration 7 the cost is 2.2594 and gradient is -2.2445 \n",
      "iteration 8 the cost is 1.9003 and gradient is 1.8977 \n",
      "iteration 9 the cost is 1.6158 and gradient is -1.5695 \n",
      "iteration 10 the cost is 1.4028 and gradient is 1.2693 \n",
      "iteration 11 the cost is 1.2518 and gradient is -1.0036 \n",
      "iteration 12 the cost is 1.1504 and gradient is 0.7756 \n",
      "iteration 13 the cost is 1.0857 and gradient is -0.5856 \n",
      "iteration 14 the cost is 1.0466 and gradient is 0.4319 \n",
      "iteration 15 the cost is 1.0242 and gradient is -0.3110 \n",
      "iteration 16 the cost is 1.0119 and gradient is 0.2186 \n",
      "iteration 17 the cost is 1.0056 and gradient is -0.1499 \n",
      "iteration 18 the cost is 1.0025 and gradient is 0.1003 \n",
      "iteration 19 the cost is 1.0011 and gradient is -0.0654 \n",
      "iteration 20 the cost is 1.0004 and gradient is 0.0416 \n",
      "iteration 21 the cost is 1.0002 and gradient is -0.0258 \n",
      "iteration 22 the cost is 1.0001 and gradient is 0.0155 \n",
      "iteration 23 the cost is 1.0000 and gradient is -0.0091 \n",
      "iteration 24 the cost is 1.0000 and gradient is 0.0052 \n",
      "iteration 25 the cost is 1.0000 and gradient is -0.0029 \n",
      "iteration 26 the cost is 1.0000 and gradient is 0.0016 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# define the cost function\n",
    "def cost_function(x):\n",
    "    return x ** 2 + 4 * x + 5\n",
    "\n",
    "\n",
    "# compute the gradient of the cost function\n",
    "def gradient(x):\n",
    "    return 2 * x + 4\n",
    "\n",
    "\n",
    "# set the initial position and the learning rate\n",
    "x = i = 0\n",
    "learning_rate = 1.0\n",
    "max_iter = 100\n",
    "\n",
    "# create a list to store the cost at each iteration\n",
    "costs = []\n",
    "\n",
    "# repeat the gradient descent update until convergence\n",
    "while True:\n",
    "    # compute the gradient and the cost at the current position\n",
    "    grad = gradient(x)\n",
    "    cost = cost_function(x)\n",
    "\n",
    "    # store the cost in the list\n",
    "    costs.append(cost)\n",
    "\n",
    "    # decrease the learning rate over time\n",
    "    learning_rate *= 0.99\n",
    "\n",
    "    # check for convergence\n",
    "    if np.abs(grad) < 1e-3 or i > max_iter:\n",
    "        break\n",
    "    print(f\"iteration {i} the cost is {cost:.4f} and gradient is {grad:.4f} \")\n",
    "    # update the position using the gradient and the learning rate\n",
    "    x -= learning_rate * grad\n",
    "    i += 1\n",
    "\n",
    "# plot the cost at each iteration\n",
    "plt.plot(costs)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the cost decreases smoothly and consistently, indicating that the adaptive learning rate helps the algorithm to converge to the minimum in a stable and efficient manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q3 Can you provide an example of how gradient descent is used in the training of a machine learning model?\n",
    "\n",
    "1. gradient descent is commonly used in the training of machine learning models, such as linear regression, logistic regression, or neural networks. Here is an example of how gradient descent can be used to train a linear regression model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.222\n",
      "Slope: 1.937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# generate some synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1)\n",
    "\n",
    "# create a linear regression model and fit it using gradient descent\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "# print the model coefficients\n",
    "print(f\"Intercept: {model.intercept_[0]:.3f}\")\n",
    "print(f\"Slope: {model.coef_[0][0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 the cost is 1.0000001686649442 and gradient is [[-3.52157528]] \n",
      "iteration 20 the cost is 1.0000001686649442 and gradient is [[-0.22375505]] \n",
      "iteration 40 the cost is 1.0000001686649442 and gradient is [[-0.07915877]] \n",
      "iteration 60 the cost is 1.0000001686649442 and gradient is [[-0.05951851]] \n",
      "iteration 80 the cost is 1.0000001686649442 and gradient is [[-0.04749786]] \n",
      "iteration 100 the cost is 1.0000001686649442 and gradient is [[-0.03801759]] \n",
      "iteration 120 the cost is 1.0000001686649442 and gradient is [[-0.03043387]] \n",
      "iteration 140 the cost is 1.0000001686649442 and gradient is [[-0.02436311]] \n",
      "iteration 160 the cost is 1.0000001686649442 and gradient is [[-0.01950332]] \n",
      "iteration 180 the cost is 1.0000001686649442 and gradient is [[-0.01561293]] \n",
      "iteration 200 the cost is 1.0000001686649442 and gradient is [[-0.01249856]] \n",
      "Intercept: 1.282\n",
      "Slope: 1.825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate some synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1)\n",
    "\n",
    "# set the initial coefficients and the learning rate\n",
    "w = np.random.randn(2, 1)\n",
    "learning_rate = 0.1\n",
    "i = 0\n",
    "max_iter = 200\n",
    "# repeat the gradient descent update until convergence\n",
    "while True:\n",
    "    # compute the predictions and the errors\n",
    "    y_pred = np.dot(X, w[1:]) + w[0]\n",
    "    errors = y_pred - y\n",
    "\n",
    "    # compute the mean squared error\n",
    "    mse = np.mean(errors ** 2)\n",
    "\n",
    "    # check for convergence\n",
    "    if mse < 1e-3 or i > max_iter:\n",
    "        break\n",
    "\n",
    "    # compute the gradient of the error with respect to the coefficients\n",
    "    grad = 2 * np.dot(errors.T, X) / X.shape[0]\n",
    "    if i % 20 == 0:\n",
    "        print(f\"iteration {i} the cost is {cost} and gradient is {grad} \")\n",
    "    # update the coefficients using the gradient and the learning rate\n",
    "    w[1:] -= learning_rate * grad\n",
    "    w[0] -= learning_rate * errors.sum() / X.shape[0]\n",
    "    i += 1\n",
    "\n",
    "# print the final coefficients\n",
    "print(f\"Intercept: {w[0][0]:.3f}\")\n",
    "print(f\"Slope: {w[1][0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q8 How do you determine when the gradient descent algorithm has converged to a minimum?\n",
    "1. There are several ways to determine when the gradient descent algorithm has converged to a minimum. One common approach is to monitor the value of the objective function at each iteration of the algorithm, and to stop the algorithm when the change in the objective function becomes small or negligible. For example, you could stop the algorithm when the absolute change in the objective function is less than a certain threshold, such as 0.001 or 0.0001. This approach is based on the assumption that the objective function will decrease monotonically as the algorithm progresses, and that it will eventually reach a minimum and stop changing significantly.\n",
    "2. Another approach is to monitor the magnitude of the gradient of the objective function at each iteration, and to stop the algorithm when the gradient becomes small or zero. This approach is based on the fact that at a minimum of the objective function, the gradient will be zero, and that the magnitude of the gradient will decrease as the algorithm approaches a minimum. To implement this approach, you could calculate the magnitude of the gradient at each iteration, and stop the algorithm when the magnitude is less than a certain threshold, such as 0.001 or 0.0001.\n",
    "3. Finally, you could use a combination of these two approaches, by monitoring both the change in the objective function and the magnitude of the gradient, and stopping the algorithm when either of these conditions is met. This can provide a more robust stopping criterion for the algorithm, as it takes into account both the value of the objective function and the gradient at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9766782196107501\n"
     ]
    }
   ],
   "source": [
    "# define the objective function\n",
    "def objective(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "\n",
    "# define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return 2 * x + 2\n",
    "\n",
    "\n",
    "# set the initial value for the model parameter\n",
    "x = 5\n",
    "\n",
    "# set the learning rate\n",
    "alpha = 1\n",
    "\n",
    "# set the convergence threshold for the objective function\n",
    "eps = 0.001\n",
    "\n",
    "# iterate until convergence\n",
    "while True:\n",
    "    # calculate the gradient at the current value of x\n",
    "    grad = gradient(x)\n",
    "\n",
    "    alpha *= 0.99\n",
    "\n",
    "    # update the model parameter using gradient descent\n",
    "    x = x - alpha * grad\n",
    "\n",
    "    # calculate the objective function at the new value of x\n",
    "    obj = objective(x)\n",
    "\n",
    "    # check if the objective function has converged\n",
    "    if abs(obj - objective(x + alpha * grad)) < eps:\n",
    "        break\n",
    "\n",
    "# print the final value of the model parameter\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0003102808178828\n"
     ]
    }
   ],
   "source": [
    "# define the objective function\n",
    "def objective(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "\n",
    "# define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return 2 * x + 2\n",
    "\n",
    "\n",
    "# set the initial value for the model parameter\n",
    "x = 5\n",
    "\n",
    "# set the learning rate\n",
    "alpha = 1\n",
    "\n",
    "# set the convergence threshold for the gradient\n",
    "eps = 0.001\n",
    "\n",
    "# iterate until convergence\n",
    "while True:\n",
    "    # calculate the gradient at the current value of x\n",
    "    grad = gradient(x)\n",
    "    alpha *= 0.99\n",
    "    # check if the gradient has converged\n",
    "    if np.linalg.norm(grad) < eps:\n",
    "        break\n",
    "\n",
    "    # update the model parameter using gradient descent\n",
    "    x = x - alpha * grad\n",
    "\n",
    "# print the final value of the model parameter\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q9 Can you discuss some strategies for accelerating the convergence of gradient descent?\n",
    "1. Using a larger learning rate: The learning rate determines how fast the algorithm updates the model parameters, and it is a crucial hyperparameter in gradient descent. Increasing the learning rate can accelerate the convergence of the algorithm, but it can also make the algorithm more unstable and prone to divergence. To strike a balance between convergence speed and stability, you can use a larger learning rate initially, and then decrease it over time, as the algorithm approaches the minimum. This approach can accelerate the convergence of the algorithm without sacrificing stability.\n",
    "2. Using a momentum term: Another way to accelerate the convergence of gradient descent is to use a momentum term, which adds a fraction of the previous update to the current update. This term can help the algorithm escape from local minima and avoid oscillations, by providing a small amount of \"inertia\" to the updates. To implement this strategy, you can add a momentum term to the update rule, which is a weighted sum of the current gradient and the previous update. This term can be adjusted using a momentum hyperparameter, which determines the weight of the previous update.\n",
    "3. Using a line search: A line search is an optimization technique that is used to find the optimal step size for the gradient descent algorithm. Rather than using a fixed learning rate, a line search algorithm computes the step size that maximizes the decrease in the objective function at each iteration. This can improve the convergence of the algorithm, by ensuring that the step size is always optimal, rather than using a fixed learning rate that may not be optimal for all iterations. To implement a line search, you can use a one-dimensional optimization algorithm, such as Brent's method or the golden-section search, to find the optimal step size at each iteration.\n",
    "4. Using a warm start: A warm start is a strategy that uses the solution from a previous optimization problem as the starting point for a new optimization problem. This can accelerate the convergence of the algorithm, by providing a better starting point that is closer to the global minimum. To implement a warm start, you can save the solution from the previous optimization problem, and use it as the starting point for the new optimization problem. This approach can be useful when solving a sequence of similar optimization problems, or when the initial value of the model parameters is not close to the global minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9022909227250784\n"
     ]
    }
   ],
   "source": [
    "# define the objective function\n",
    "def objective(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "\n",
    "# define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return 2 * x + 2\n",
    "\n",
    "\n",
    "# set the initial value for the model parameter\n",
    "x = 5\n",
    "\n",
    "# set the initial learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# set the convergence threshold for the objective function\n",
    "eps = 0.001\n",
    "\n",
    "# iterate until convergence\n",
    "while True:\n",
    "    # calculate the gradient at the current value of x\n",
    "    grad = gradient(x)\n",
    "\n",
    "    # update the model parameter using gradient descent\n",
    "    x = x - alpha * grad\n",
    "\n",
    "    # calculate the objective function at the new value of x\n",
    "    obj = objective(x)\n",
    "\n",
    "    # check if the objective function has converged\n",
    "    if abs(obj - objective(x + alpha * grad)) < eps:\n",
    "        break\n",
    "\n",
    "    # decrease the learning rate over time\n",
    "    alpha = alpha / 2\n",
    "\n",
    "# print the final value of the model parameter\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9898531895508867\n"
     ]
    }
   ],
   "source": [
    "# define the objective function\n",
    "def objective(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "\n",
    "# define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return 2 * x + 2\n",
    "\n",
    "\n",
    "# set the initial value for the model parameter\n",
    "x = 5\n",
    "\n",
    "# set the learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# set the momentum hyperparameter\n",
    "beta = 0.9\n",
    "\n",
    "# initialize the momentum term\n",
    "momentum = 0\n",
    "\n",
    "# set the convergence threshold for the objective function\n",
    "eps = 0.001\n",
    "\n",
    "# iterate until convergence\n",
    "while True:\n",
    "    # calculate the gradient at the current value of x\n",
    "    grad = gradient(x)\n",
    "\n",
    "    # update the momentum term\n",
    "    momentum = beta * momentum + grad\n",
    "\n",
    "    # update the model parameter using gradient descent with momentum\n",
    "    x = x - alpha * momentum\n",
    "\n",
    "    # calculate the objective function at the new value of x\n",
    "    obj = objective(x)\n",
    "\n",
    "    # check if the objective function has converged\n",
    "    if abs(obj - objective(x + alpha * grad)) < eps:\n",
    "        break\n",
    "\n",
    "# print the final value of the model parameter\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7\n"
     ]
    }
   ],
   "source": [
    "# define the objective function\n",
    "def objective(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "# define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return 2*x + 2\n",
    "\n",
    "# define the line search algorithm\n",
    "def line_search(x, alpha, beta):\n",
    "    # initialize the step size\n",
    "    t = 1\n",
    "\n",
    "    # iterate until convergence\n",
    "    while True:\n",
    "        # calculate the gradient at the current value of x\n",
    "        grad = gradient(x)\n",
    "\n",
    "        # compute the new value of x using the step size\n",
    "        x_new = x - t * grad\n",
    "\n",
    "        # calculate the objective function at the new value of x\n",
    "        obj_new = objective(x_new)\n",
    "\n",
    "        # calculate the objective function at the current value of x\n",
    "        obj = objective(x)\n",
    "\n",
    "        # compute the change in the objective function\n",
    "        delta = obj - obj_new\n",
    "\n",
    "        # check if the step size is optimal\n",
    "        if 0 <= delta <= alpha * t * np.linalg.norm(grad)**2:\n",
    "            break\n",
    "\n",
    "        # decrease the step size\n",
    "        t = beta * t\n",
    "\n",
    "    # return the optimal step size\n",
    "    return t\n",
    "\n",
    "# set the initial value for the model parameter\n",
    "x = 5\n",
    "\n",
    "# set the convergence threshold for the objective function\n",
    "eps = 0.001\n",
    "\n",
    "# iterate until convergence\n",
    "while True:\n",
    "    # calculate the gradient at the current value of x\n",
    "    grad = gradient(x)\n",
    "\n",
    "    # use the line search algorithm to find the optimal step size\n",
    "    t = line_search(x, 0.1, 0.5)\n",
    "\n",
    "    # update the model parameter using gradient descent with a line search\n",
    "    x = x - t * grad\n",
    "\n",
    "    # calculate the objective function at the new value of x\n",
    "    obj = objective(x)\n",
    "\n",
    "    # check if the objective function has converged\n",
    "    if abs(obj - objective(x + t * grad)) < eps:\n",
    "        break\n",
    "\n",
    "# print the final value of the model parameter\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A line search algorithm helps to speed up the operation of gradient descent by finding the optimal step size for the gradient descent update at each iteration. By using the optimal step size, the algorithm can make larger and more efficient updates, which can accelerate the convergence of the algorithm.\n",
    "\n",
    "In general, the step size in gradient descent determines how much the model parameters are updated at each iteration, and it plays a crucial role in the convergence of the algorithm. If the step size is too small, the algorithm will converge slowly, because the updates will be too small to make significant progress. On the other hand, if the step size is too large, the algorithm may diverge, because the updates will be too large and the objective function may increase instead of decrease. To avoid these problems, it is important to use the optimal step size for each iteration, and this is what a line search algorithm does.\n",
    "\n",
    "A line search algorithm computes the step size that maximizes the decrease in the objective function at each iteration. This is done by evaluating the objective function at different step sizes, and selecting the step size that provides the largest decrease in the objective function. By using this optimal step size, the gradient descent algorithm can make larger and more efficient updates, which can accelerate the convergence of the algorithm. Additionally, a line search algorithm can help the algorithm avoid divergence, because it ensures that the step size is always sufficient to decrease the objective function. This is why a line search algorithm can help to speed up the operation of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# define the objective function\n",
    "def objective(w, lambda_):\n",
    "    return w**2 + 2*w + 1 + lambda_ * abs(w)\n",
    "\n",
    "# set the regularization hyperparameter\n",
    "lambda_ = 0.5\n",
    "\n",
    "# generate a range of values for the model parameter\n",
    "w = np.linspace(-5, 5, 100)\n",
    "\n",
    "# calculate the objective function for each value of w\n",
    "obj = objective(w, lambda_)\n",
    "\n",
    "# plot the objective function\n",
    "plt.plot(w, obj)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('f(w)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "import numpy as np\n",
    "\n",
    "# define the objective function\n",
    "def objective(w, lambda_):\n",
    "    return w**2 + 2*w + 1 + lambda_ * abs(w)\n",
    "\n",
    "# set the regularization hyperparameter\n",
    "lambda_ = 0.5\n",
    "\n",
    "# generate a range of values for the model parameter\n",
    "w = np.linspace(-5, 5, 100)\n",
    "\n",
    "# calculate the objective function for each value of w\n",
    "obj = objective(w, lambda_)\n",
    "\n",
    "# create a figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the objective function\n",
    "ax.plot(w, obj)\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('f(w)')\n",
    "\n",
    "# create a slider for the regularization hyperparameter\n",
    "slider_lambda = plt.axes([0.25, 0.1, 0.65, 0.03])\n",
    "slider = Slider(slider_lambda, 'lambda', 0.0, 1.0, valinit=lambda_)\n",
    "\n",
    "# update the objective function when the regularization hyperparameter is changed\n",
    "def update(val):\n",
    "    lambda_ = slider.val\n",
    "    obj = objective(w, lambda_)\n",
    "    ax.clear()\n",
    "    ax.plot(w, obj)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('f(w)')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# connect the slider to the update function\n",
    "slider.on_changed(update)\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# define the objective function\n",
    "def objective(w, lambda_):\n",
    "    return w[0]**2 + w[1]**2 + 2*w[0] + 2*w[1] + 1 + lambda_ * (abs(w[0]) + abs(w[1]))\n",
    "\n",
    "# set the regularization hyperparameter\n",
    "lambda_ = 0.5\n",
    "\n",
    "# generate a grid of values for the model parameters\n",
    "w0, w1 = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "\n",
    "# calculate the objective function for each pair of (w0, w1) values\n",
    "obj = objective(np.array([w0, w1]), lambda_)\n",
    "\n",
    "# create a figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# create an Axes3D instance and add it to the figure\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot the objective function\n",
    "surf = ax.plot_surface(w0, w1, obj, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "# customize the plot\n",
    "ax.set_zlim(0, 100)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.set_xlabel('w0')\n",
    "ax.set_ylabel('w1')\n",
    "ax.set_zlabel('f(w0, w1)')\n",
    "\n",
    "# add a color bar\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}